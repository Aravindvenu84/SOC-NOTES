# Log Parsing and Normalization Before Ingestion

## Introduction

Logs generated by security devices, operating systems, applications, and cloud services are typically unstructured or semi-structured. Before these logs can be used for detection and correlation in a SIEM, they must be parsed, normalized, enriched, and indexed. This process ensures consistency, searchability, and compatibility with detection rules.

---

## 1. Log Collection

Logs are first collected from various sources such as:

- Windows Event Logs
- Linux Syslog
- Firewalls
- IDS/IPS systems
- Web servers
- Cloud platforms (AWS, Azure, GCP)
- Endpoint security tools

Collection methods include:

- Agents (Splunk UF, Elastic Agent)
- Syslog forwarding
- API-based ingestion
- Log shippers (Filebeat, Logstash)

At this stage, logs are still in raw format.

---

## 2. Log Parsing

### Definition

Parsing is the process of extracting structured fields from raw log messages.

Raw logs are often plain text strings. Parsing converts them into searchable key-value pairs.

### Example

Raw Log:

```
Feb 12 10:22:31 server1 sshd[2456]: Failed password for root from 192.168.1.10 port 51432 ssh2
```

Parsed Fields:

- timestamp: Feb 12 10:22:31
- hostname: server1
- service: sshd
- action: Failed password
- user: root
- source_ip: 192.168.1.10
- port: 51432

### Parsing Techniques

- Regular Expressions (Regex)
- Grok patterns (Logstash)
- JSON field extraction
- XML parsing
- Vendor-specific parsers (Splunk Field Extractions, QRadar DSM)

Parsing ensures that each meaningful component of the log becomes a structured and searchable field.

---

## 3. Log Normalization

### Definition

Normalization converts parsed fields into a standardized schema so logs from different sources follow a unified format.

Different devices use different naming conventions:

| Vendor A | Vendor B | Normalized Field |
|-----------|-----------|-----------------|
| src_ip    | source    | source_ip       |
| user      | username  | user            |
| evt_time  | timestamp | timestamp       |

### Common Schemas

- Splunk CIM (Common Information Model)
- Elastic ECS (Elastic Common Schema)
- QRadar DSM mapping
- OCSF (Open Cybersecurity Schema Framework)

### Purpose of Normalization

- Enables cross-source correlation
- Allows detection rules to work universally
- Improves search accuracy
- Reduces vendor dependency

Without normalization, detection logic becomes inconsistent and unreliable.

---

## 4. Log Enrichment

After normalization, logs may be enriched with additional context:

- GeoIP lookup
- Threat intelligence feeds
- Asset classification
- User directory mapping
- Vulnerability data

Enrichment enhances detection accuracy and investigation efficiency.

---

## 5. Indexing and Storage

Once logs are parsed and normalized:

- They are indexed for fast search.
- Stored according to retention policies.
- Made available for correlation and alerting.

At this stage, logs are ready for SOC operations.

---

## Conclusion

Parsing extracts structured data from raw logs. Normalization standardizes those fields across multiple sources. Together, they enable accurate detection, correlation, and effective SIEM operations.

 
