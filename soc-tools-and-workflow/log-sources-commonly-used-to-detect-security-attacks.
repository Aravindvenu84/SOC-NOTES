# Log Parsing and Normalization Before Ingestion

## Introduction

Logs generated by security devices, operating systems, applications, and cloud services are typically unstructured or semi-structured. Before these logs can be used for detection and correlation in a SIEM, they must be parsed, normalized, enriched, and indexed. This process ensures consistency, searchability, and compatibility with detection rules.

---

## 1. Log Collection

Logs are first collected from various sources such as:

- Windows Event Logs
- Linux Syslog
- Firewalls
- IDS/IPS systems
- Web servers
- Cloud platforms (AWS, Azure, GCP)
- Endpoint security tools

Collection methods include:

- Agents (Splunk UF, Elastic Agent)
- Syslog forwarding
- API-based ingestion
- Log shippers (Filebeat, Logstash)

At this stage, logs are still in raw format.

---

## 2. Log Parsing

### Definition

Parsing is the process of extracting structured fields from raw log messages.

Raw logs are often plain text strings. Parsing converts them into searchable key-value pairs.

### Example

Raw Log:

```
Feb 12 10:22:31 server1 sshd[2456]: Failed password for root from 192.168.1.10 port 51432 ssh2
```

Parsed Fields:

- timestamp: Feb 12 10:22:31
- hostname: server1
- service: sshd
- action: Failed password
- user: root
- source_ip: 192.168.1.10
- port: 51432

### Parsing Techniques

- Regular Expressions (Regex)
- Grok patterns (Logstash)
- JSON field extraction
- XML parsing
- Vendor-specific parsers (Splunk Field Extractions, QRadar DSM)

Parsing ensures that each meaningful component of the log becomes a structured and searchable field.

---

## 3. Log Normalization

### Definition

Normalization converts parsed fields into a standardized schema so logs from different sources follow a unified format.

Different devices use different naming conventions:

| Vendor A | Vendor B | Normalized Field |
|-----------|-----------|-----------------|
| src_ip    | source    | source_ip       |
| user      | username  | user            |
| evt_time  | timestamp | timestamp       |

### Common Schemas

- Splunk CIM (Common Information Model)
- Elastic ECS (Elastic Common Schema)
- QRadar DSM mapping
- OCSF (Open Cybersecurity Schema Framework)

### Purpose of Normalization

- Enables cross-source correlation
- Allows detection rules to work universally
- Improves search accuracy
- Reduces vendor dependency

Without normalization, detection logic becomes inconsistent and unreliable.

---

## 4. Log Enrichment

After normalization, logs may be enriched with additional context:

- GeoIP lookup
- Threat intelligence feeds
- Asset classification
- User directory mapping
- Vulnerability data

Enrichment enhances detection accuracy and investigation efficiency.

---

## 5. Indexing and Storage

Once logs are parsed and normalized:

- They are indexed for fast search.
- Stored according to retention policies.
- Made available for correlation and alerting.

At this stage, logs are ready for SOC operations.

---

## Conclusion

Parsing extracts structured data from raw logs. Normalization standardizes those fields across multiple sources. Together, they enable accurate detection, correlation, and effective SIEM operations.

---

# Step-by-Step Log Integration Workflow

## Introduction

Log integration into a SIEM follows a structured workflow to ensure reliable ingestion, proper formatting, and effective detection capability.

---

## Step 1: Identify Log Source

- Determine the device or application.
- Identify log format (syslog, JSON, XML, proprietary).
- Identify transport method (agent, syslog, API).

---

## Step 2: Configure Log Forwarding

- Install agent or configure syslog settings.
- Define SIEM destination IP and port.
- Ensure secure communication (TLS if required).

---

## Step 3: Validate Log Reception

- Confirm logs reach the SIEM.
- Check ingestion metrics.
- Validate timestamps and source identification.

Logs are still raw at this stage.

---

## Step 4: Configure Parsing Rules

- Apply regex or grok patterns.
- Extract key fields such as IP address, username, event type.
- Test parsing accuracy.

---

## Step 5: Apply Normalization

- Map extracted fields to a common schema (CIM, ECS, DSM).
- Validate field consistency across multiple sources.

---

## Step 6: Apply Enrichment

- Add GeoIP data.
- Integrate threat intelligence.
- Attach asset or user context.

---

## Step 7: Index and Store Logs

- Assign appropriate index.
- Configure retention policy.
- Optimize storage tiers.

---

## Step 8: Implement Detection Rules

- Create correlation rules.
- Define thresholds and alert triggers.
- Test with sample data.

---

## Step 9: Monitor and Tune

- Review false positives.
- Optimize parsing and normalization if needed.
- Adjust detection thresholds.

---

## Workflow Summary

1. Log generation
2. Log collection
3. Parsing
4. Normalization
5. Enrichment
6. Indexing
7. Detection
8. Monitoring and tuning

---

## Conclusion

A structured log integration workflow ensures consistency, scalability, and accurate detection in SIEM environments. Proper parsing and normalization directly impact the effectiveness of threat detection and SOC performance.
